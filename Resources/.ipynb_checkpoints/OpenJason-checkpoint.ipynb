{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "import os\n",
    "\n",
    "# data_directory: str\n",
    "#     Path to a directory to store data.\n",
    "data_directory = '.'\n",
    "\n",
    "# install_missing_packages: bool\n",
    "#     A flag indicating if missing packages should be automatically installed\n",
    "install_missing_packages = True\n",
    "\n",
    "# use_conda: bool\n",
    "#     A flag indicating if conda should be used for software installation.\n",
    "#     If False, pip will be used. The default is to use conda if jupyter\n",
    "#     is run in a conda environment.\n",
    "use_conda = 'CONDA_EXE' in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for missing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def check_package(package, pip_pkg: str = None, conda_pkg: str = None):\n",
    "    \"\"\"Check if a given package is installed. If missing install\n",
    "    it (if global flag `install_missing_packages` is True) either with\n",
    "    pip or with conda (depending on `use_conda`).\n",
    "    \"\"\"\n",
    "    if importlib.util.find_spec(package) is not None:\n",
    "        return  # ok, package is already installed\n",
    "\n",
    "    if not install_missing_packages:\n",
    "        raise RuntimeError(f\"{package} is not installed!\")\n",
    "\n",
    "    if use_conda:\n",
    "        import conda.cli\n",
    "        conda.cli.main('conda', 'install',  '-y', conda_pkg or package)\n",
    "    else:\n",
    "        import subprocess\n",
    "        import sys            \n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_pkg or package])\n",
    "        \n",
    "# This is to exit cells without error tracebacks (cosmetic purpose)\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the required environment (skip if already done)\n",
    "\n",
    "Running the following cell will create a file graphs.yml that can be used to setup a conda environment containing the required packages. If you already downloaded the file from my GitHub, skip the next cell and create the env directly from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing graphs.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile graphs.yml\n",
    "name: graphs\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.6\n",
    "  - jupyter\n",
    "  - imageio\n",
    "  - imageio-ffmpeg\n",
    "  - matplotlib\n",
    "  - scikit-image\n",
    "  - opencv\n",
    "  - networkx\n",
    "  - pandas\n",
    "  - statsmodels\n",
    "  - scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Creation\n",
    "To create the environment, open the terminal, go to the directory where you stored the graphs.yml file (the directory of the notebook) and type\n",
    "`conda env create -f graphs.yml`\n",
    "After running this command you have to activate the environment (Linux/MacOS: `conda activate graphs`, Windows: activate graphs) and then reopen the notebook in that environment.\n",
    "### JypiterNotebook integration\n",
    "Now comes the step to set this conda environment, to have the enviroment as a kernel in your jypiterNotebook. On the same terminal into which yoou have activated your enviroment type:\n",
    "- `conda install -c anaconda ipykernel`\n",
    "- `python -m ipykernel install --user --name=graphs`\n",
    "- Reload your notebook \n",
    "- Select graphs as your kernel \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import glob\n",
    "import scipy.cluster.vq as clusters\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from pandas.plotting import autocorrelation_plot as AC_plot \n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from skimage.filters import gaussian\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_DATA_PATH = '/Users/tracysanchezpacheco/Documents/HumanAnalysis/HumanAnalysis'\n",
    "GIT_DATA_PATH = '/Users/tracysanchezpacheco/Documents/HumanAnalysis/HumanAnalysis/Data Exploration'\n",
    "GIT_PROCESSED_DATA_PATH = '/Users/tracysanchezpacheco/Documents/HumanAnalysis/HumanAnalysis/Results'\n",
    "GIT_GRAPH_PATH = '/Users/tracysanchezpacheco/Documents/HumanAnalysis/HumanAnalysis/Results/Graphs/'\n",
    "RESSOURCES_PATH = '/Users/tracysanchezpacheco/Documents/HumanAnalysis/HumanAnalysis/Resources/'\n",
    "\n",
    "# Reset the Datapath since the data is not yet on Git, comment out if data is on Git \n",
    "DATA_PATH = GIT_DATA_PATH\n",
    "PROCESSED_DATA_PATH = GIT_PROCESSED_DATA_PATH\n",
    "GRAPH_DATA_PATH = GIT_GRAPH_PATH\n",
    "\n",
    "# Getting the Folder without hidden files in ascending order \n",
    "DATA_FOLDER = sorted([f for f in os.listdir(DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "GIT_PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(GIT_PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "#houselist \n",
    "house_file = RESSOURCES_PATH + 'building_collider_list.csv'\n",
    "try:\n",
    "    houselist = pd.read_csv(house_file)\n",
    "except:\n",
    "    print('HouseList could not be loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000_Expl_S_000_ET_1_1632396670,86291.json',\n",
       " '000_TaskDataFrame__EyeTracking1_1632662251,68487.json',\n",
       " '999_EyeValidation_1632408958.04931_TA.json',\n",
       " '999_PointingTask_Randomization_1632408793.09208.json',\n",
       " '999_TaskDataFrame_1632410484.07538.json',\n",
       " '999_TaskDataFrame_1632410493.17054_OnQuit.json',\n",
       " '999_TaskDataFrame__EyeTracking1_1632410484.06745.json',\n",
       " '999_TaskDataFrame__EyeTracking2_1632410493.17004_OnQuit.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all subject IDs from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 999]\n"
     ]
    }
   ],
   "source": [
    "subIDs = []\n",
    "for sub in DATA_FOLDER:\n",
    "    if sub[0].isdigit():\n",
    "        subIDs.append(int(sub[0:3])) #First four characters are numbers \n",
    "    else:\n",
    "        pass\n",
    "subIDs = np.unique(subIDs)\n",
    "print(subIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0 started - 1/2 subjects\n",
      "\tSubject 0 Filename is not valid!\n",
      "Subject 999 started - 2/2 subjects\n",
      "\tSubject 999 Filename is not valid!\n"
     ]
    }
   ],
   "source": [
    "# --------- Preparation ---------\n",
    "\n",
    "Session_save_bool = False # set to True if you want to save each individual session as csv\n",
    "Exploration_save_bool = True # set to True if you want to save the complete exploration as csv\n",
    "Subject_Info_save_bool = True # set to True if you want to save the subject data as csv\n",
    "subcount = 0\n",
    "graffity_replaced_count = 0\n",
    "removed_body_hits = 0\n",
    "removed_nohouse_hits = 0\n",
    "\n",
    "landmarks = ['Castle-TaskBuilding_56',\n",
    "             'Church-TaskBuilding_16',\n",
    "             'HighSilo-TaskBuilding_49',\n",
    "             'Windmill-TaskBuilding_10_1',\n",
    "             'crane_1',\n",
    "             'crane_2']\n",
    "\n",
    "garages_to_stay = ['Garage_185', \n",
    "                   'Garage_224', \n",
    "                   'Garage_235',  \n",
    "                   'Garage_86', \n",
    "                   'Garage_98']\n",
    "\n",
    "\n",
    "# column name list for dataframe\n",
    "col_names =  ['Session',\n",
    "              'timeStampDataPointStart',\n",
    "              'timeStampDataPointEnd',\n",
    "              'hitObjectColliderName', \n",
    "              'ordinalOfHit',\n",
    "              'BitMask',\n",
    "              'hitPointOnObject.x',\n",
    "              'hitPointOnObject.y',\n",
    "              'hitPointOnObject.z',\n",
    "              'hitObjectColliderBoundsCenter.x',\n",
    "              'hitObjectColliderBoundsCenter.y',\n",
    "              'hitObjectColliderBoundsCenter.z',\n",
    "              'transformed_collidercenter_x',\n",
    "              'transformed_collidercenter_y'\n",
    "              'hmdPosition.x',\n",
    "              'hmdPosition.y',\n",
    "              'hmdPosition.z']\n",
    "\n",
    "\n",
    "NoHit_dict = {'hitObjectColliderName': 'NoHit',\n",
    "              'ordinalOfHit': '1'}\n",
    "\n",
    "\n",
    "subject_cols = ['SubjectID',\n",
    "                'Sessions',\n",
    "                'ET_Sessions',\n",
    "                'Total Rows Combined',\n",
    "                'Removed Body Hits', \n",
    "                'Removed Graffiti Hits', \n",
    "                'Replaced NoHouse Hits',\n",
    "                'DataLoss BitMask', \n",
    "                'DataLoss NoHits',\n",
    "                'DataLoss Combined']\n",
    "\n",
    "subject_data_df = pd.DataFrame(columns=subject_cols)\n",
    "\n",
    "\n",
    "# --------------------------- MAIN PART ---------------------------\n",
    "\n",
    "\n",
    "# --------- first layer - subject loop ---------\n",
    "\n",
    "for subject in subIDs:\n",
    "    subcount +=1\n",
    "    print('Subject ' \n",
    "          + str(subject) \n",
    "          + ' started - ' \n",
    "          + str(subcount) \n",
    "          + '/' \n",
    "          + str(len(subIDs)) \n",
    "          + ' subjects')\n",
    "    \n",
    "    # Create empty dataframe for later concatenation\n",
    "    complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    # get the data files according to the subject, ignoring OnQuit files\n",
    "    subject_folder = sorted([f for f in DATA_FOLDER \n",
    "                             if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "                            key=str.lower) \n",
    "    \n",
    "    # the following works as long as the data name format is as follows:\n",
    "    # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "    folder_files = list()\n",
    "       \n",
    "    # loop through the subject folder and save all numbers\n",
    "    for file in subject_folder:\n",
    "        folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "    # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "    try:\n",
    "        SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "    except:\n",
    "        print('\\tSubject ' \n",
    "              + str(subject)\n",
    "              + ' Filename is not valid!')\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>timeStampDataPointStart</th>\n",
       "      <th>timeStampDataPointEnd</th>\n",
       "      <th>hitObjectColliderName</th>\n",
       "      <th>ordinalOfHit</th>\n",
       "      <th>BitMask</th>\n",
       "      <th>hitPointOnObject.x</th>\n",
       "      <th>hitPointOnObject.y</th>\n",
       "      <th>hitPointOnObject.z</th>\n",
       "      <th>hitObjectColliderBoundsCenter.x</th>\n",
       "      <th>hitObjectColliderBoundsCenter.y</th>\n",
       "      <th>hitObjectColliderBoundsCenter.z</th>\n",
       "      <th>transformed_collidercenter_x</th>\n",
       "      <th>transformed_collidercenter_yhmdPosition.x</th>\n",
       "      <th>hmdPosition.y</th>\n",
       "      <th>hmdPosition.z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Session, timeStampDataPointStart, timeStampDataPointEnd, hitObjectColliderName, ordinalOfHit, BitMask, hitPointOnObject.x, hitPointOnObject.y, hitPointOnObject.z, hitObjectColliderBoundsCenter.x, hitObjectColliderBoundsCenter.y, hitObjectColliderBoundsCenter.z, transformed_collidercenter_x, transformed_collidercenter_yhmdPosition.x, hmdPosition.y, hmdPosition.z]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_exploration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_json('Test.json')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_records(results[\"trials\"], columns = ['trialId', 'someRandomInformation',  'timeTrialMeasurementStarted', 'timeTrialMeasurementStopped', 'dataPoints'])\n",
    "df\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df['dataPoints']\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "datapoints = ['timeStampDataPointStart']\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "new_df = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in df['dataPoints']],ignore_index=True)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "new_df.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#df2 = pd.json_normalize(results['trials'])\n",
    "pd.json_normalize(df['dataPoints'])\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "df2\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame.from_records(df[\"dataPoints\"], columns = ['eyePositionCombinedWorld'])\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "df2 = df.dataPoints\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "df2n = pd.json_normalize(df['dataPoints'])\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "results.trials['dataPoints']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
